import logging
from collections import defaultdict
from typing import List, Tuple, Dict, Optional
import json

import networkx as nx
from langchain_core.documents import Document

# Reuse functions from sibling modules
from .create_vectors import (
    get_vector_store_path_for_url,
    load_existing_vector_store,
)
from .create_graphs import get_graph_path

logger = logging.getLogger(__name__)



def retrieve_from_group(
    group_name: str, 
    query: str, 
    k: int, 
    config: dict
) -> List[Tuple[Document, float]]:
    """
    Retrieves the top k documents from all vector stores associated with a group_name.
    Right now it is not relevant, but for the future if we encounter multiwebsite documentation,
    like LangChain, we can use this function to retrieve from all websites at once.

    Args:
        group_name: The key from config['documentation_urls'].
        query: The user's query string.
        k: The total number of documents to return.
        config: The loaded configuration dictionary.

    Returns:
        A list of tuples, each containing a Document and its similarity score,
        sorted by score (descending).
    """
    logger.info(f"Retrieving top {k} docs for query in group '{group_name}'")
    
    if group_name not in config.get('documentation_urls', {}):
        logger.error(f"Group name '{group_name}' not found in config['documentation_urls']")
        return []

    urls_in_group = config['documentation_urls'][group_name]
    if not urls_in_group:
        logger.warning(f"No URLs configured for group '{group_name}'")
        return []

    all_results: List[Tuple[Document, float]] = []

    for url in urls_in_group:
        index_path = get_vector_store_path_for_url(config, url)
        logger.debug(f"Loading vector store for URL: {url} from path: {index_path}")
        
        vector_store = load_existing_vector_store(index_path, config)
        
        if vector_store:
            logger.info(f"Vector store loaded for URL: {url}")
            try:
                # Perform similarity search. Assuming score is similarity (higher is better).
                results_with_scores: List[Tuple[Document, float]] = vector_store.similarity_search_with_score(query, k=k)
                logger.debug(f"Retrieved {len(results_with_scores)} results from {url}")
                all_results.extend(results_with_scores)
            except Exception as e:
                logger.error(f"Error during similarity search for {url}: {e}", exc_info=True)
        else:
            logger.warning(f"Could not load vector store for URL: {url} at path: {index_path}")

    # Sort all collected results by score (descending) and take top k
    all_results.sort(key=lambda x: x[1], reverse=True)
    
    top_k_results = all_results[:k]
    logger.info(f"Returning {len(top_k_results)} combined results for group '{group_name}'")
    for doc, score in top_k_results:
        logger.info(f"Retrieved from {doc.metadata.get('source')}")
    return top_k_results


def _organize_initial_results(
    initial_results: List[Tuple[Document, float]]
) -> Tuple[Dict[str, Dict], set, set]:
    """Organizes initial retrieval results."""
    # we want to keep only unique documents
    scores: Dict[str, Dict] = {}
    doc_ids_in_set = set()

    for doc, vec_sim in initial_results:
        doc_id = doc.id # generated by the vector store, but in graph we use it too
        source = doc.metadata.get('source')
        if not doc_id:
            # This case should be less likely now, but check anyway
            logger.warning(f"Document object missing 'id' attribute: {doc.page_content[:100]}...")
            continue
        if not source:
             logger.warning(f"Document missing 'source' in metadata: {doc.page_content[:100]}...")
             continue
             
        scores[doc_id] = {'doc': doc, 'vector_sim': vec_sim}
        doc_ids_in_set.add(doc_id)
   
    logger.debug(f"Prepared {len(doc_ids_in_set)} unique documents")
    return scores, doc_ids_in_set

def _get_top_level_url_for_source(source_url: str, config: dict) -> Optional[str]:
    # Find the top-level URL corresponding to a specific source URL
    best_match_url = None
    max_len = -1

    # Iterate through all defined top-level URLs in the config
    for url_list in config.get('documentation_urls', {}).values():
        for top_level_url in url_list:
            # Check if the top-level URL is a prefix of the source URL
            if source_url.startswith(top_level_url):
                # Keep the longest matching prefix
                if len(top_level_url) > max_len:
                    max_len = len(top_level_url)
                    best_match_url = top_level_url
    
    if best_match_url is None:
        logger.warning(f"Could not find a matching top-level URL for source: {source_url}")
        
    return best_match_url

def _load_graphs_for_group(
    group_name: str,
    config: dict
) -> Dict[str, Optional[nx.DiGraph]]:
    """Loads NetworkX graphs for the top-level URLs associated with a group."""
    graphs: Dict[str, Optional[nx.DiGraph]] = {}
    
    if group_name not in config.get('documentation_urls', {}):
        logger.error(f"Group name '{group_name}' not found in config['documentation_urls']")
        return graphs
        
    top_level_urls = config['documentation_urls'][group_name]
    
    for top_level_url in top_level_urls:
        graph_file_path = get_graph_path(top_level_url, config) # Use helper to get path
        logger.debug(f"Attempting to load graph for URL: {top_level_url} from path: {graph_file_path}")
        try:
            # Reuse the existing load_graph_for_url logic, but provide the correct path
            # We need to load json and build graph here, assuming load_graph_for_url expects the URL not path
             with open(graph_file_path, "r") as f:
                data = json.load(f)
             graphs[top_level_url] = nx.node_link_graph(data)
             logger.debug(f"Successfully loaded graph for top-level URL: {top_level_url}")
        except FileNotFoundError:
            logger.warning(f"Graph file not found for URL: {top_level_url} at path: {graph_file_path}. Skipping graph contribution.")
            graphs[top_level_url] = None
        except Exception as e:
            logger.error(f"Error loading graph for {top_level_url} from {graph_file_path}: {e}", exc_info=True)
            graphs[top_level_url] = None
    return graphs

def _augment_scores_with_graph(
    scores_data: Dict[str, Dict], 
    doc_ids: set, 
    loaded_graphs: Dict[str, Optional[nx.DiGraph]],
    config: dict,
    k_expanded: int,
    eval_run: bool = False
) -> Dict[str, Dict[str, float]]:
    """
    Augments the scores of the initial retrieval results with graph-based scores.
    We do not re-rank, because it will bring complexity. 
    We just add documents that are positionally close to the initial results.
    
    Args:
        scores_data: A dictionary mapping document IDs to their scores and documents.
        doc_ids: A set of document IDs to augment.
        loaded_graphs: A dictionary mapping top-level URLs to their loaded NetworkX graphs.
        config: The loaded configuration dictionary.
        k_expanded: The number of documents to expand the final result to
        eval_run: Whether this is an evaluation run.
    """
    augmented_docs = defaultdict(tuple)

    for doc_id in doc_ids:
        source_url = scores_data[doc_id]['doc'].metadata.get('source')
        if not source_url:
            logger.warning(f"Document ID {doc_id} missing source metadata, cannot determine graph.")
            continue
        # add original documents to the augmented docs
        augmented_docs[doc_id] = (
            scores_data[doc_id]['vector_sim'],
            source_url
        )

        # Find the correct top-level URL/graph for this document's source
        top_level_url = _get_top_level_url_for_source(source_url, config)
        if top_level_url is None:
            continue # Error already logged by helper
            
        G = loaded_graphs.get(top_level_url)

        if G is not None and doc_id in G:
            # Consider both successors and predecessors for neighbors in the directed graph
            neighbors = set(G.successors(doc_id)) | set(G.predecessors(doc_id))
            logger.info(f"Document ID {doc_id} has {len(neighbors)} neighbors.")
            
            for neighbor_id in neighbors:
                # final weight of the neighbor depents on the importance of the original doc_id
                edge_weight = G.get_edge_data(doc_id, neighbor_id, {}).get('weight', 0.0)
                augmented_docs[neighbor_id] = (
                    edge_weight * scores_data[doc_id]['vector_sim'],
                    top_level_url
                )
        else:
             if G is not None and doc_id not in G:
                 logger.warning(f"Document ID '{doc_id}' not found in loaded graph for source '{source_url}'.")

    logger.debug(f"Calculated raw graph context scores for {len(augmented_docs)} documents.")
    docs_with_graph_scores = sorted(augmented_docs.items(), key=lambda x: x[1][0], reverse=True)[:k_expanded]
    if not eval_run:
        # Return the top k_expanded results based on the augmented score
        logger.info(f"Returning {min(len(docs_with_graph_scores), k_expanded)} augmented scores.")
        doc_ids_by_url = defaultdict(list)
        for doc_id, (score, url) in docs_with_graph_scores:
            doc_ids_by_url[url].append(doc_id)
        logger.info(f"Returning {len(doc_ids_by_url)} unique sources.")
        return doc_ids_by_url
    else:
        return docs_with_graph_scores

def retrieve_from_group_with_graph(
    group_name: str,
    query: str,
    k: int,
    config: dict,
    k_expanded_multiplier: int = 2,
    eval_run: bool = False
) -> List[Document]:
    """
    Retrieves documents using vector similarity and graph-based score augmentation.

    Args:
        group_name: The key from config['documentation_urls'].
        query: The user's query string.
        k: The final number of documents to return.
        config: The loaded configuration dictionary.
        k_expanded_multiplier: Factor to expand k for initial retrieval.
        eval_run: Whether this is an evaluation run.

    Returns:
        A list of re-ranked Documents.
    """
    logger.info(f"Starting retrieval with graph re-ranking for group '{group_name}'")
    # vector_store = load_existing_vector_store(get_vector_store_path_for_url(config, group_name), config)
    # 1. Initial Expanded Retrieval
    logger.info(f"Performing initial retrieval for top {k} documents.")
    expanded_results = retrieve_from_group(group_name, query, k, config)

    if not expanded_results:
        logger.warning("Initial retrieval yielded no results. Returning empty list.")
        return []

    # 2. Prepare Data Structures
    scores_data, doc_ids_in_set = _organize_initial_results(expanded_results)
    if not doc_ids_in_set:
        logger.warning("No valid documents found after preparation. Returning empty list.")
        return []
       
    # 3. Load Graphs for the entire group
    loaded_graphs = _load_graphs_for_group(group_name, config)
    logger.info(f"Loaded {len(loaded_graphs)} graphs for group '{group_name}'")

    # 4. Calculate Raw Graph Scores
    doc_ids_by_url = _augment_scores_with_graph(
        scores_data, 
        doc_ids_in_set, 
        loaded_graphs,
        config,
        k_expanded_multiplier * k,
        eval_run
    )
    if not eval_run:
        return _collate_documents_by_source(doc_ids_by_url, config)
    else:
        return _get_raw_chunks_for_eval(doc_ids_by_url, config)
        # return doc_ids_by_url


def _collate_documents_by_source(doc_ids_by_url, config) -> List[Document]:
    """Groups documents by source, sorts by position, and collates content."""
    docs_to_return = defaultdict(list)
    for url, doc_ids in doc_ids_by_url.items():
        vector_store_path = get_vector_store_path_for_url(config, _get_top_level_url_for_source(url, config))
        logger.info(f"Loading vector store from {vector_store_path}")
        vector_store = load_existing_vector_store(vector_store_path, config)
        for doc_id in doc_ids:
            document = vector_store.docstore._dict.get(doc_id)
            if document:
                source = document.metadata.get('source')
                docs_to_return[source].append(document)
    final_docs = []
    for source, documents in docs_to_return.items():
        logger.info(f"Collating documents for source: {source}")
        sorted_docs = sorted(documents, key=lambda x: x.metadata.get('position', float('inf')))
        joint_content = "\n\n".join([d.page_content for d in sorted_docs])
        final_docs.append(Document(page_content=joint_content, metadata=sorted_docs[0].metadata))
    return final_docs


def _get_raw_chunks_for_eval(doc_ids_by_url, config) -> List[Document]:
    """Groups documents by source, sorts by position, and collates content."""
    final_chunks = []
    for doc_id, (score, url) in doc_ids_by_url:
        if url != 'https:https://ai.pydantic.dev':
            url = _get_top_level_url_for_source(url, config)
        vector_store_path = get_vector_store_path_for_url(config, url)
        logger.info(f"Loading vector store from {vector_store_path}")
        vector_store = load_existing_vector_store(vector_store_path, config)
        final_chunks.append((vector_store.docstore._dict[doc_id], score))
    return final_chunks
